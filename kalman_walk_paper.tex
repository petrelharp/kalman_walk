\documentclass{article}
\usepackage{amsmath, amssymb, color, xcolor}
\usepackage{graphicx, wrapfig, float, caption, dsfont, bbm}
\usepackage{fullpage}
\usepackage[backref=page, hidelinks, colorlinks=true, citecolor=blue!60!black!100]{hyperref}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, shapes}
\usepackage{caption, subcaption}
\usepackage{natbib} % gives us \citet: Author (year) and \citep: (Author; year)
\usepackage{authblk}

\usepackage{multicol}

% for comments in margins or not
\newif\ifmargincomments
\margincommentstrue
% \margincommentstrue
\ifmargincomments
    \usepackage{todonotes}
    \usepackage[left=1cm,right=6.5cm,top=3cm,bottom=3cm,nohead,nofoot,marginparwidth=6cm]{geometry}
    \newcommand{\plr}[1]{\todo[color=blue!25]{#1}}
    \newcommand{\js}[1]{\todo[color=green!25]{#1}}
    % inline comment version
    \newcommand{\plri}[1]{{\color{blue}\it #1}}
\else
    \newcommand{\plr}[1]{{\color{blue}\it #1}}
    \newcommand{\plri}[1]{\plr{#1}}
\fi

\newcommand{\jss}[1]{{\color{olive}\it #1}}
% \newcommand{\ddt}{\frac{d}{dt}}
\newcommand{\ddt}{\dot}
\newcommand{\ro}{{ro}}
\newcommand{\nro}{{\bar{r}o}}
\newcommand{\rno}{{r\bar{o}}}
\newcommand{\nrno}{{\bar{r}\bar{o}}}
\newcommand{\reachable}{\mathcal{R}}
\newcommand{\unobservable}{\bar{\mathcal{O}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\pda}{\frac{\partial}{\partial A_{ij}}}
\newcommand{\ind}{\mathds{1}}

\newcommand{\calA}{\mathcal{A}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\diag}{\text{diag}}
\newcommand{\1}{\mathbbm{1}}

% fitness as a fn of distance
\newcommand{\fit}{\mathcal{F}}
% fitness as a fn of A
\newcommand{\fitx}{\mathcal{F}}
% set of optimal coefficients
\newcommand{\optx}{\mathcal{X}}
% optimal phenotype
\newcommand{\optph}{\Phi_0}
% distance in phenotype space
\newcommand{\dph}{d}
% incompatibility
\newcommand{\Incompat}{\mathcal{I}}

\DeclareMathOperator{\spn}{span}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\begin{document}

\section*{Rapid speciation despite conservation of phenotype}
{\centering
Joshua S. Schiffman$^{\dagger}$ \qquad Peter L. Ralph$^{\dagger \ddagger}$ \\
$^{\dagger}$University of Southern California, Los Angeles, California \qquad 
$^{\ddagger}$University of Oregon, Eugene, Oregon \\
\texttt{jsschiff@usc.edu} \qquad 
\texttt{plr@uoregon.edu}
\\
}

% Other title ideas:
%
% More than one way to grow a cat: regulatory network drift and speciation
% 
% Rapid speciation despite conservation of phenotype
%
% More than one way to grow a cat: regulatory network redundancy and speciation
% 
% Neutral network drift leads to incompatibilities on the same time scale as genetic drift
% 
% More than one way to grow a cat: can network drift lead to speciation?
% 
% How fast does network drift create incompatibilities?
% 
% Beyond the snowball: a quantitative model of incompatibility accumulation
% 
% Systems drift and speciation: more than one way to grow a cat
% 
% An explicit model of neutral regulatory network evolution, with applications
% to the rate of accumulation of hybrid incompatibility
% 
% Evolutionary conservation of phenotype does not entail conservation of the underlying
% molecular mechanism leading to rapid speciation
%
% The evolution of phenotype-invariant gene networks rapidly leads to hybrid incompatibiliy
%
% Evolutionary network rewiring can rapidly lead to hybrid incompatibility despite
% phenotypic conservation
%
% Hybrid incompatibilities can evolve rapidly due to developmental systems drift
%
% Evolutionary systems theory and speciation 

\begin{abstract}
We introduce an analytical theory to study the evolution of biological systems, such as gene regulatory networks. The evolutionary conservation of phenotype under selective and environmental stasis does not necessitate conservation of the underlying mechanism, as distinct molecular pathways can realize identical phenotypes. Here we give an exact expression for the set of all linear mechanisms with identical phenotypes, and expect evolution under neutrality to explore this set. We employ a quantitative genetic approach to model evolution under neutrality as a random process over the set of all phenotype-invariant mechanisms: only mutational tweaks to the pathway that leave the phenotype invariant are optimally fit. We show that there is never a unique linear system architecture for any phenotype and that the evolutionary exploration of these distinct and mutationally connected mechanisms can lead to the rapid accumulation of hybrid incompatibilities between allopatric populations and thus lead to the rapid formation of new species -- in fewer generations than there are breeding individuals in a population.
\end{abstract}

\plr{Additional ideas to consider adding:}
\begin{itemize}
    %\item Haldane's rule (easy point in discussion: makes F1s look like F2s)
    %\item add point about F1: quartic and F2: quadratic to results and maybe abstract/discussion
    \item hybrid vigor (need to do calculation)
    \item discuss linearity, linearization, and canalization in introduction
    %\item obtain estimates of variation in $A$ from thermodynamic occupancy model
\end{itemize}

\plri{Note: in the \LaTeX source I'm putting in semantic linebreaks, so it's easy to edit and move around phrases and ideas.}

\plri{Need to come up with a consistent term for ``the $A_{ij}$''s. -- ``regulatory coefficients''? ``genotype''?}

%%%%%%%%%%%%%%%%%%%%%%
%\begin{multicols}{2}
\section*{Introduction}

Bridging the gulf between an organism's genome and phenotype is a poorly understood and complex molecular machinery. 
Progress in a suite of biological subdisciplines is stalled by our general lack of understanding of this molecular machinery: with respect to both its function and evolution. 
  \plr{Saying "stalled" reflects negatively on other fields - rephrase?} \js{I don't think it reflects negatively. Even if it did, as long as it's a true statement it should be fine.}.
There does exist a growing body of data on the evolutionary histories and molecular characterizations of particular gene regulatory networks \citep{jaeger2011gap, davidson2006gene, israel2016comparative}, as well as thoughtful verbal and conceptual models \citep{true2001developmental, gwagner1, weiss2000phenogenetic, edelman2001degeneracy}. 
  \plr{I don't think the HW example will speak to our audience.  Also, better to say why math is good rather than why not-math is bad.} \js{deleted HW. I don't think we need to write an anodyne -- the current sentence is true enough.}
However, verbal theories are often insufficient, if not downright misleading \citep{servedio2014not}. 
This is especially pertinent given the staggering complexity and scope of contemporary research programs. 
This outlook necessitates the advancement of conceptual frameworks of such precision, 
only mathematics will suffice, as models allow the development of concrete numerical predictions. 
%Previously it has been suggested that any idealized study of evolution is incomplete 
%without a mathematically sufficient description of the genotype, phenotype, and transformation from one to the other \citep{Lewontin1974genetic}.
\plr{probably some more recent Wagner papers in this line as well?}
%Here we outline an analytical theory to study the evolution of biological systems, borrowing insight and methods from control engineering, systems identification, and dynamical systems theory. 

%Presently, we focus on the neutral evolution of genetic regulatory networks. That is, we analytically describe the set of all linear gene networks (of any size) that produce identical phenotypes -- and the evolutionary paths connecting them. In the idealized case of a perfectly adapted population, constant selection, and a static environment, we observe evolution under the ``conservation of phenotype'' as a Brownian motion over phenotypically-invariant network-space. This analysis provides insight to the mechanisms and parameters important for understanding developmental systems drift, network rewiring, evolvability, epistasis, and speciation, as well as the tenuous connection between network architecture and function.


%It is commonly taught that an organism's genome contains the heritable material 
%that natural selection filters and that an organism's phenotype directly determines its
%evolutionary fitness. Between genotype and phenotype is an often complicated and poorly
%undrstood molecular machinery -- and it is a major goal common to many different disciplines
%within the life sciences to elucidate its form, function, and evolution. These aims are
%delicately intertwined and a comphrehensive understanding of a system's evolution requires
%an understanding of its function and vice versa.

The molecular machinery, interacting with the environment, and bridging genotype to phenotype
can be mathematically described as a dynamical system -- or a system of differential equations \citep{jaeger2015comet}.
 Movement in this direction is ongoing, as researchers have begun to study 
the evolution of both abstract \citep{wagner1994evolution, wagner1996does,  siegal2002waddington, bergman2003evolutionary, draghi2015robustness} and empirically inspired computational and mathematical models of gene regulatory networks (GRNs) \citep{mjolsness1991connectionist, jaeger2004dynamic, maria1, vitaly1, vitaly2, crombach2016gap, wotton2015quantitative, chertkova2017insilico}. If we allow the reasonable assumption that the genotype-phenotype map can be represented as a system of differential equations, we can immediately discuss its evolution and function in a much more mechanistic, yet general, manner. 

In some fields that seek to fit parametric models to experimental data, such as control
theory, chemical engineering, and statistics, it is well known that mathematical models
can fundamentally be \emph{unidentifiable} and/or \emph{indistinguishable} -- meaning that 
there can be uncertainty about an inferred model's parameters or even its claims about
causal structure, even with access to complete and perfect data \citep{bellman1970structural, grewal1976identifiability, walter1984structural}. 
%\plr{not clear what you mean by "actually"}\js{I think we discussed it's clear enough for an intro?}
Models with different parameter schemes, or even different mechanics can be equally accurate, 
but still not \emph{actually} agree with what is being modelled. 
In control theory, where electrical circuits and mechanical systems are often the focus, 
it is understood that there can be an infinite number of ``realizations'', 
or ways to reverse engineer the dynamics of a black box,
even if all possible input and output experiments on the black box are performed \citep{kalman1963mathematical, anderson1966equivalence, zadeh1976linear}. 
In chemical engineering, those who study chemical reaction networks sometimes refer to the fundamental
unidentifiability of these networks as ``the fundamental dogma of chemical kinetics'' \citep{craciun2008identifiability}. 
In computer science, this is framed as the relationship among processes that simulate one another \citep{van2004equivalence}.  
\plr{here you are jumping to evolution -- haven't said yet why evolution "can explore" these -- needs to say explicitly what we mean by neutral here}
Although this may frustrate the occasional engineer or scientist, viewed from another angle,
the concepts of unidentifiability and indistinguishability can provide a starting point for
thinking about externally equivalent systems -- systems that evolution can explore, so long
as the parameters and structures can be realized biologically. In fact, evolutionary
biologists who study convergent versus parallel evolution, homology, and analogy are very familiar with such functional symmetries; 
macroscopically identical phenotypes in even very closely related species can in fact be divergent at the molecular and sequence level 
\citep{true2001developmental, tsong2006evolution, hare2008sepsid, vierstra2014mouse, stergachis2014conservation, taylor2016diverse, matsui2015regulatory}.

In this paper we outline a theoretical framework to study the evolution of biological systems. 
  \plr{rather than "we expect" maybe say that the framework could be applied to non-neutral evolution}\js{fixed}
Presently, we focus solely on a neutral scenario, 
that is where phenotype is conserved over evolutionary time. 
however, this framework could be applied to a wider set of evolutionary scenarios. 
\plr{here's the definition of neutral, but it's not quite right?} \js{how's that?}
  
We present an analytical description of the set of all linear biological systems with identical phenotypes 
\plr{We need something here saying what we mean by "gene network architecture".  Also, I don't think we claim anything for "all biological systems".} \js{added the word ``linear.''}
-- that is we describe the set of all gene network architectures that yield identical phenotypes, and show that all linear biological systems can, in principal, can undergo systems drift. 
In the neutral case, this set describes a manifold that evolution explores leaving phenotype invariant with respect to mutation, 
and predicts that if two populations become reproductively isolated, hybrid incompatibility can occur, despite the absence of adaptation, directional selection, or environmental change. 
Speciation typically occurs on timescales approximately on the order of $N_{e}$ generations, where $N_{e}$ is the effective population size.

\section*{Gene Networks as Linear Dynamical Systems}

\plr{Below we mix general language with language specific to GRNs, like switching between "internal state" and "transcription factor concentration.  
Rather than keep making comments like "(or some other system)",
we could say up top that this applies to other situations, but to make it concrete we'll talk about regulatory networks.}

%  \jss{Organisms' phenotypes are constructred by gene by gene by environment interactions. Here we simply define the phenotype to be the organismal temporal molecular dynamics directly under natural selection. The \emph{what}, \emph{when}, and \emph{where}, of an organism's molecules that are physiologically or otherwise relevant for survival. 
%   Thus we say that some function $\phi(t)$ is a phenotype where, 
%  \begin{align}
%    \phi(t) = \int_{0}^{\infty} h(t) u(t) dt  ,
%  \end{align}
%  and $h(t)$ is the \emph{impulse response} of the system and $u(t)$ is the \emph{input} function, both functions of time $t$. The input can be interpreted as the environment, as initial conditions, or otherwise, depending on the biological specifics under study. The phenotype is a convolution of both the impulse response and the input, as allowed by our assumption of linearity. 
%
%  Essentially the phenotype $\phi(t)$ is a consequence of an organism's specific gene by gene interactions, given by $h(t)$, reacting further with the local environment, given by $u(t)$. 
%
%  We describe the impulse response as, 
%  \begin{align}
%    h(t) = C e^{A t} B
%  \end{align}
%  where $A$ is a gene regulatory network (although this model can be generalized to other biological networks) -- a square matrix, and $B$ filters and translates the input to the system. The form of $B$ determines precisely how the state of the external environment influences the internal gene network. $C$ filters and translates the dynamics of the system and precesily determines the output, that is, what is visible to selection. 
%
%  Generally $A$ can be any real $n \times n$ matrix, $B$ any $n \times \ell$, and $C$ any $\ell \times n$ dimensional matrix. Each $i$th row of $A$ describes the \emph{cis}-regulatory module for gene $i$, and each $j$th entry, the specific regulatory influence of gene $j$ on gene $i$. 
%
%  Although $\phi(t)$ describes the phenotype given an input, $h(t)$ describes the phenotype subject only to an impulse -- an input present initially and absent immediately thereafter. Typically, a system $\Sigma$, is defined as,
%  \begin{align}
%    \Sigma = \left\{ \begin{array}{ll} \dot{\kappa}(t) &= A \kappa(t) + B u(t) \\ \phi(t) &= C \kappa(t) \end{array} \right.
%  \end{align}
%  Variables have the same identities as described above and $\kappa(t)$ is a vector of molecule concentrations at time $t$. Therefore the molecular concentrations at a specific time are completely determined by the input and gene by gene interactions. Lastly, a portion and/or combination of these molecules, $\phi(t)$, are ``observed'' by selection (this is in constrast to $\kappa(t)$ -- the \emph{kryptotype} -- as it is ``hidden'' from direct selection).
%}
\js{Will rewrite this section again.} Here we outline a method to model biological systems, such as gene regulatory networks, as linear dynamical systems. 
We define an organism's ``phenotype'' to be the temporal dynamics of molecular concentrations directly relevant to survival, 
and denote by $\phi_k(t)$ the concentration of the $k^\text{th}$ element of the phenotype at time $t$, given an input; for instance $\phi_k(t)$ could describe the concentrations of a particular enzyme.
These dynamics are the result of the interconnections of 
a gene regulatory network (or some other biological system) $A$ (an $n \times n$ matrix) 
and an environmental input vector $u(t)$, and the \emph{kryptotype} $\kappa(t)$, is a list of a system's molecular concentrations at time $t$ -- or more generally the systems \emph{internal dynamics}. 
Such a system $\mathcal{S}$, is composed of two equations,
  \begin{align}
    \mathcal{S} := \left\{ \begin{array}{ll} \dot{\kappa}(t) &= A \kappa(t) + B u(t) \\ \phi(t) &= C \kappa(t) \end{array} \right.
  \end{align}
As some system dynamics may not be visible nor relevant to selection, we distinguish the \emph{kryptotype} (as it is hidden) from the \emph{phenotype} -- which is the system's \emph{external dynamics}.
The change in internal molecular concentrations as a function of time $\dot{\kappa}(t)$ is simply a function of the system's current state (the kryptotype), the system architecture $(A,B,C)$ -- such as the wiring of a gene regulatory network, and the environmental input $u(t)$ to the system. The architecture of $A$ determines exactly how the internal molecular concentrations regulate one another's expression as $A_{ij}$ is the magnitude at which transcription factor $\kappa_j(t)$ regulates transcription factor $\kappa_i(t)$. If $A_{ij} > 0$, we say that $\kappa_j$ upregulate $\kappa_i$, and if $A_{ij} < 0$, we say that $\kappa_j$ down-regulates $\kappa_i$. Furthermore, we interpret the $i^\text{th}$ row of $A$ to be the \emph{cis}-regulatory element (or promoter) for gene $i$.  
The architecture of $B$ determines how the system processes the environmental intput, and the structure of $C$ converts the internal dynamics to external dynamics. That is if $\mathcal{S}$ is a metabolic system, $C_{ij}$ is the amount at which the $j$th input metabolite $u_j$ affects the production of the $i$th enzyme concentration.  

\plr{This "Thus" needs expanding, e.g., "the solution to this equation is unique and given by" with a reference.}
Thus the phenotype is simply a convolution of the system organization and the environment, 
  \begin{align}
    \phi(t) = C e^{A t} \kappa(0) + \int_{0}^{t} C e^{A (t-s)} B u(s) ds ,
  \end{align}
where we refer to $h(t) := Ce^{A t}B$ as the system's \emph{impulse response}. 
%We will now lay out the model in more general terms.
%Suppose that the \emph{internal state} of the system
%is parameterized by the concentrations of a collection of $n$ molecular species,
%$S_1, \ldots, S_n$,
%and the vector of concentrations at time $t$ we denote $\kappa(t)=(\kappa_1(t),\ldots,\kappa_n(t))$.
%There are also $m$ ``input'' species, whose concentrations are determined
%exogenously to the system,
%and are denoted $u(t) = (u_1(t),\ldots,u_m(t))$,
%and $\ell$ ``output'' species, whose concentrations are denoted
%$\phi(t) = (\phi_1(t),\ldots,\phi_\ell(t))$.
%The output is merely a linear function of the internal state:
%\begin{align*}
%    \phi_i(t) = \sum_j C_{ij} \kappa_i(t).
%\end{align*}
%Since $\phi$ is what natural selection acts on, we refer to it as the \emph{phenotype},
%and sometimes in contrast refer to $\kappa$ as the \emph{kryptotype},
%as it is ``hidden'' from direct selection.
%The rate at which the $i^\text{th}$ species is produced
%is a weighted sum of the concentrations of the other species
%as well as the input:
%\begin{align*}
%    \ddt \kappa_i(t) = \sum_j A_{ij} \kappa_j(t) + \sum_r B_{ir} u_r(t) .
%\end{align*}
%In matrix notation, this is written more concisely as
%\begin{align} \label{eqn:lti_system}
%    \ddt \kappa(t) &= A \kappa(t) + B u(t) \\
%    \phi(t) &= C \kappa(t) .
%\end{align}


  \section*{Gene Network Drift}

%\plr{This paragraph feels like the introduction.}
%The literature is filled with detailed observations of molecular systems and their diversity. 
%\plr{Diversity doesn't imply systems drift -- only if the diverse systems are homologous.}
%There are examples of significant diversity in the networks underlying processes such as 
%circadian rhythm \citep{sancar2008intelligent}, 
%cell cycle control \citep{cross2011evolution, kearsey2003enigmatic}, 
%pattern formation \plr{cite?}, 
%and metabolism \citep{lavoie2009rearrangements, martchenko2007transcriptional, dalal2016transcriptional, christensen2011unique, hartl2007induction, alam2013aspergillus}. 
Despite a symmetry in functionality or phenotype systems can often differ, sometimes substantially, at the molecular level. 
How many different mechanisms have the same function? 

Gene regulatory networks with identical phenotypes do not necessarily have identical kryptotypes.
Any linear and minimal system (a gene network) 
-- minimal, informally meaning that the system's phenotype is achieved with the fewest possible number of genes 
-- has an identical impulse response and therefore an identical phenotype given an identical input $u(t)$, up to a change of coordinates. 
\plr{"has an identical phenotype" is not what you mean to say here}
\js{edited -- does that work?}
  \begin{equation}
    \begin{aligned}
      h(t) &= C e^{A t} B 
      = C V^{-1} V e^{A t} V^{-1} V B \\
      &= C V^{-1} e^{V A V^{-1} t} V B 
      = \bar{C} e^{\bar{A} t} \bar{B}
    \end{aligned}
  \end{equation}
\plr{what are we calling these -- not "biological system" -- that's too broad}\js{how about ``molecular systems''?}
Two molecular systems, $\mathcal{S} = \{ A, B, C \}$, and $\bar{\mathcal{S}} = \{\bar{A} = VAV^{-1}, \bar{B} = VB, \bar{C} = CV^{-1} \}$, 
have the same phenotype, for all possible inputs, if they are related by a change of coordinates. 
\plr{is this the place to say "and only if, in the minimal dimension"?}

%Although systems may not be identifiable beyond a change of coordinates, at present we are primarily interested in a subset of these systems. 
%That is, systems that not only have equivalent external dynamics, but also equivalent input and output relationships. 
%\plr{clarify that you mean actually having the same $B$ and $C$. (at first I read "input and output relationship" as the mapping $u \to \phi$),
%    i.e. behaviour across many possible inputs.}
%Formally, this means systems related by a change of coordinates (any invertible matrix $V$) that leaves $B$ and $C$ invariant:
%  \begin{align}
%    VB &= B \implies \bar{B} = B \\
%    CV &= C \implies \bar{C} = C
%  \end{align}
%\plr{this sentence lacks an object (or some sentence part?)}
%In other words systems with varying gene regulatory network architectures yet identical selection pressures, environment, and phenotype.

%Define $V(\tau)$ as the parameterized change of coordinates matrix that preserves $B$ and $C$, with $\tau$ a vector of free parameters. The set of \emph{all} phenotypically invariant (minimal) gene networks is, 
 % \begin{align}
 %   A(\tau) = V(\tau) A(0) V^{-1}(\tau) ,
 % \end{align}
%and a \emph{Linear Evolutionary System} is, 
%\plr{why is this "evolutionary"? It is the thing that we're thinking about evolution of, but that doesn't make it evolutionary.}
Therefore, if a system is minimal, the set of all phenotypically identical systems $\mathcal{S}(V)$ can be parameterized as,
    \begin{align}
      \mathcal{S}(V) := \left\{ \begin{array}{ll} V\dot{\kappa}(t) &= VA V^{-1} \kappa(t) + VB u(t) \\ \phi(t) &= C V^{-1} \kappa(t) \end{array} \right.
  \end{align}
  where $V$ is any invertible matrix and it's elements are free parameters. 
More generally,
we denote by $\calA_n(A_0)$ the set of all $n$-dimensional systems equivalent to $A_0$:
\begin{equation} \label{eqn:equivalence}
  \begin{aligned}
  \calA_n(A_0) 
      &= \left\{
          A : C e^{At} B = C e^{A_0 t} B \; \text{for}\; t \ge 0 
      \right\}  \\
      &= \left\{
          A : C A^k B = C A_0^k B \; \text{for}\; 1 \le k \le n-1 
      \right\} .
  \end{aligned}
\end{equation}
Equivalence of the two characterizations follows from the Cayley-Hamilton theorem.
\plr{To make sense of this we need to say how $C$ and $B$ change with $n$.}
Usually, the dimension $n$ and the reference system $A_0$ is implicit and we write only $\calA$.

\plr{should say more about this decomposition here.}
Regardless of minimality, two systems, even in different dimensions, can have identical external dynamics if they are in $\mathcal{A}$. This set can be completely parameterized using the \emph{Kalman decomposition} \ref{apx:kalman}; the set of all linear gene regulatory networks with equivalent phenotypes can be precisely defined. Note that this implies that there is always more than one possible gene regulatory network architecture per phenotype, however we can analytically describe the complete set. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection*{Speciation}

%Define reproduction in diploids as first, the recombination of unlinked genes to make gametes, and second, as the averaging of two individual parental gametes to produce an offspring. Assuming parental populations are both phenotypically identical and genetically homogenous within each population, first generation hybrids (F1s) can be computed by averaging the two parental gene networks. Second generation hybrids (F2s) can be computed by first swapping the genes between the two parental gene networks, and next averaging these hybrid gametes.

%Speciation, or the formation of new species, is often a consequence of reprdouctive incompatibility among populations -- usually via hybrid sterility or inviability. To discuss speciaiton we first need to describe how reproduction is modelled.
 %A diploid first generation hybird's ($F_1$'s) genetic regulation is a consequence of its two genomes, as a copy of genes and regulatory sequences from each parent is present.
%\plr{should have a stronger justification for the assertion that $F_1$ is the arithmetic average of the parents than "Thus we say"}
%A diploid organism contains two gene network copies: one from each parent. As such, a diploid's system dynamics is computed by averaging the coefficients of both systems. To reproduce, each organism passes on a recombined haploid gene network to its offspring. Haploid gene networks swap system matrix rows randomly between its own two networks. 
%\plr{I don't think we say that $A$ is a "gene network architecture".}
%Therefore the phenotypic dynamics typical of an $F_{1}$ cross between two allopatric populations will be determined simply by averaging it's two parental genomes. However, in second generation hybrid crosses $F_2$ between allopatric populations, first new haploid systems will be formed by recombination -- in the process shuffling and combining regulatory coefficients from allopatric populations -- and then brought together to form a diploid.

%\begin{align}
%    F_{1}(\tau, \hat{\tau}) &= \frac{ A(\tau) + A(\hat{\tau}) }{2}  .
%   h(t, F_{1}) &= C e^{\left( F_{1}\left( \tau, \hat{\tau} \right) t \right)} B
%\end{align}
%\begin{align*}
% G\left(r, \tau, \hat{\tau}\right) = Q(r) A( \tau ) + \left( I - Q(r) \right) A( \hat{\tau} )
%\end{align*}
%\begin{align}
%    F_{2}(i, j) = \frac{G (i) + G(j)}{2}
%\end{align}

  %\begin{align*}
  %  h\left(t, F_{2}\left( r, r' \right)\right) = C e^{\left( F_{2}\left(r, r' \right) t \right)} B
  %\end{align*}

%Fitness $\fit$ is a function (usually a Guassian) of an organism's weighted phenotypic distance from an optimum,
%  $\fit\left( \widehat{\phi}(t) \right) = \exp \left(- \int_{0}^{t} \frac{\rho(s)}{\beta} \left\lVert \phi(s) - \widehat{\phi}(s) \right\rVert^{2} ds  \right)$
%where $\rho(t)$ is some weighting function and $\beta$ is a parameter.

%We quantitatively describe the degree of incompatibility between two populations $P_{1}$ and $P_{2}$ as
%\begin{align*}
%      \Incompat = \frac{2 \left\langle \fit \left(\phi_{F_{1}}\right) \right\rangle}{\left\langle \fit \left(\phi_{P_{1}}\right) \right\rangle +  \left\langle \fit \left(\phi_{P_{2}}\right) \right\rangle} ,
%\end{align*}
%where angled brackets imply averaging.  \plr{averaging over what? And, this is as others define - cite.}

%\plr{This is not part of the definition of $\mathcal{I}$:}
%\js{should we delete this example ($A(0) \times A(2)$ hybrids)? I like how clean it is -- but I'm not sure it's that important in this context. It's a good proof of principle though.}
%\jss{$F_1$s created by crossing phenotypically equivalent oscillators $A(0)$ and $A(2)$ have a phenotype of $\phi_{F_{1}}(t) = e^{t}$, 
%in contrast to both parents, who have $\phi_{P_{1}}(t) = \phi_{P_{2}}(t) = \sin(t) + \cos(t)$. 
%The hybrid phenotype is significantly different (it does not oscillate and increases infinitely) despite the phenotypic equivalence of the parents.
%%\plr{don't need these to be big equations.}
%%\begin{align*}
%%  \fit \left(\phi_{P_{1}}\right) = \fit \left( \phi_{P_{2}} \right) &= 1 \\
%%  \fit\left(\phi_{F_{1}}\right) &= 0 \\
%%  \mathcal{I} &= 0
%%\end{align*}
%
%%Thus if populations $1$ and $2$ are homogeneous $A(0)$ and $A(2)$, respectively, we say that they are completely incompatible as $\mathcal{I} = 0$.
%
%  \begin{figure}[H]
%    \centering
%    \includegraphics[width=0.5\textwidth, height=0.25\paperheight]{expF1}
%    \caption{$F_1$ hybrid (orange) and parental (blue) phenotypic oscillator dynamics for an $A(0)$ by $A(2)$ cross. The hybrid fails to oscillate and exhibits qualitatively different dynamics.}
%  \end{figure}
%  }
%%     \begin{example}[F1 Reproductive Incompatibility in an Oscillating Gene Network]
%%       DMI examples...
%%       \begin{figure}[H]
%%         \includegraphics[width=0.5\textwidth]{A2_A3_osci_F1_hyb}
%%       \end{figure}
%%     \end{example}
%
%%     \begin{example}[Not all Networks can Host Incompatibilities]
%%       convex sets cant have DMIs
%
%%       \begin{align*}
%%         h(t) = 2 e^{- \theta t}
%%       \end{align*}
%%       Any non-minimal system with rows summing to $\theta$ is PI. Further, these systems are closed under averaging (mating) and row swapping (meiosis), leaving all hybrids optimally fit. The set of gene matrices is affine and therefore convex.  
%%     \end{example}
%
%
%\plri{Missing: a statement about when there are neutral directions, and how many.}
%
%\plr{I think we should first put the section about quantitative drift in, and then the following section that tries to get at real values of parameters.}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Systems drift and the accumulation of incompatibilities}

At any given time, there will be a range of regulatory coefficients present in the population
due to segregating genetic variants.
Over many generations, even if selective pressures do not change,
this range of networks will shift 
as recombination, mutation, and demographic noise create new alleles and shift allele frequencies.
How much variation do we expect to find within a population?
Is this range limited by available variation or kept in check by selection?
How fast will a population explore the space of equivalent networks?
In this section we explore informally a general model for this situation,
in which a population drifts stochastically near a set of equivalent, optimal systems.
We work with a population of effective size $N_e$.

Suppose that a set $x$ of coefficients that determine a system (this is $A$ above),
produce a phenotype $\Phi(x)$ (the time course of $\phi(t)$).
There is an optimal phenotype $\optph$,
and a set $\optx$ of ``optimal'' coefficients that produce this phenotype.
Fitness depends on distance to the optimal phenotype --
we will write the ``distance'' between phenotypes $\phi$ and $\psi$ as $\dph(\phi, \psi)$,
measured so that the fitness of an organism with coefficents $x$ is $\fitx(x) = \exp(-\dph(\Phi(x),\optph)^2)$.
We will assume that the map $\Phi$ is smooth
and that the optimal set $\optx$ is locally isomorphic to $\R^m$.
\plr{say that better}

\paragraph{Offspring.}
Individuals are diploid; we assume that each haploid genome determines a set of coefficients,
and the individual's coefficients are the average of her two haploid values (no dominance).
This implies that the diploid population variance, $\sigma^2$, is one-half the haploid variance.
Each new gamete is produced from the parent's two haploid copies;
for simplicity we assume that the gamete inherits a random choice of one of the two parental copies,
and so $\sigma$ remains constant, up to a $1/N_e$ term.
A more general model including segregation variance \citep{barton_infinitesimal}
would result in the same qualitative conclusions.
\plri{but put this in the appendix?}

\paragraph{System drift}
% plr how fast does it drift
If the variation within a population of some coefficient
has standard deviation $\sigma$,
then since subsequent generations resample from this diversity,
the population mean coefficient will move a random distance of size $\sigma/\sqrt{N_e}$ per generation,
simply because this is the the standard deviation of the mean of a random sample \citep{lande_drift}.
% (This could be taken as a definition of $N_e$.)
Selection will tend to restrain this motion,
but mean movement along the optimal set $\optx$ is unconstrained.
% (although perhaps complicated by recombination load \citep{recomb_load})
The amount of variance in particular directions in coefficient space 
depend on constraints imposed by selection and 
the covariance between genetic variation between different coefficients (the $G$ matrix \citep{G_matrix}).
% covariance which may arise due to functional constraints and/or statistical linkage.
% There may well be functional constraints -- but these are not sufficiently well-known to say anything general about.
For instance,
if the variation is due to \textit{cis}-regulatory variants,
the genetic basis of each \emph{row} of $A$ likely lies within a few kilobases of tightly linked sequence,
across which a population may carry only a few common haplotypes.
However, covariance due to transiently assembled haplotypes is not expected to be stable over long periods of time --
a common \textit{cis}-regulatory haplotype of transcription factor $k$ with particularly strong binding to both $i$ and $j$
(leading to positive covariance between $A_{ik}$ and $A_{jk}$)
is no more likely to appear than one with strong binding to $i$ but particularly weak binding to $j$ (negative covariance).
(Such transient covariances may well increase the variance of the per-generation change in network mean, however \citep{barton_linkage}.)

% It therefore seems reasonable to coarsely model the time evolution of population variation in regulatory coefficients as 
% (a) a ``cloud'' of width X about the population mean, 
% which (b) moves as an unbiased Brownian motion through the set of network coefficients that give the optimal phenotype.
% In fact, the population mean will not produce exactly the optimal phenotype,
% but it will be convenient to refer to this closest point on the optimal set as ``the population mean''.

To obtain a general quantitative picture, we need to know 
$\sigma_N$ and $\sigma_S$, the standard deviations of coefficient variation along and perpendicular to $\optx$ respectively,
and $\gamma$, the scale on which phenotype changes moving away from $\optx$.
Concretely, $\gamma$ is the inverse of the derivative of $\dph(\Phi(x+u z), \Phi(x))$ 
with respect to $u$ for $x \in \optx$ and $z$ perpendicular to the tangent space at $x$.
With these parameters, a typical individual will have a fitness of around $\exp(-(\sigma_S/\gamma)^2)$.

\paragraph{Sexual reproduction}
A diploid first generation hybird's ($F_1$) genetic regulation is a consequence of its two genomes, as a copy of genes and regulatory sequences from each parent is present.
\plr{should have a stronger justification for the assertion that $F_1$ is the arithmetic average of the parents than "Thus we say"}
A diploid organism contains two gene network copies: one from each parent. As such, a diploid's system dynamics is computed by averaging the coefficients of both systems. To reproduce, each organism passes on a recombined haploid gene network to its offspring. Haploid gene networks swap system matrix rows randomly between its own two networks. 
Therefore the phenotypic dynamics typical of an $F_{1}$ cross between two allopatric populations will be determined simply by averaging it's two parental genomes. However, in second generation hybrid crosses $F_2$ between allopatric populations, first new haploid systems will be formed by recombination -- in the process shuffling and combining regulatory coefficients from allopatric populations -- and then brought together to form a diploid.

\paragraph{Hybridization}
\plr{I put F1s and F2s here, and $\Incompat$, but with less explanation than below. Merge somehow.} \js{In progress, see paragraph above.}
By the arguments above, the means of two allopatric populations separated for $T$ generations
will be a distance of order $2\sigma_N \sqrt{T/N_e}$ apart along $\optx$.
A population of $F_1$ hybrids has one haploid genome from each,
whose coefficients are averaged,
and so will have mean system coefficients at the midpoint between their means,
and variance equal to $\sigma$.
Each $F_2$ hybrid will be homozygous for one parental allele on averge at half of the loci in the genome,
so the distribution of $F_2$s will have mean at the average of the two populations,
as before,
but variance equal to $\sigma^2 + z^2/2$, where $z$ is the distance between the parental populations.
These are depicted in figure \ref{fig:conceptual_fig}.

\begin{figure}
\centering
\includegraphics{figures/conceptual_fig}
\caption{
    \label{fig:conceptual_fig}
    A conceptual figure of the fitness consequences of hybridization:
    axes represent system coefficients (i.e., entries of $A$);
    the line of optimal system coefficients is down in black;
    dotted lines give phenotypic distances to the optimum.
    Two pairs of parental populations are shown in black, along the optimum;
    a hypothetical population of $F_1$s are shown for each in red,
    and the distribution of one type of $F_2$ is shown in purple
    (other types of $F_2$ are not shown).
    Solid lines depict the distance of the $F_2$ to optimum.
    \plri{Should show all types of F2? would be messy.}
}
\end{figure}

\plri{improve figure by putting labels on from the following}
Suppose that two populations have drifted independently to differ by $z$,
and that $z$ is of the same order as $\sigma$ but is smaller than $\gamma$.
The mean $F_1$ is the average of the parental means,
and since the first-order terms in the Taylor series vanish,
has phenotype differing from the optimum by a distance of order $\|z\|^2$ 
(see appendix \ref{apx:away_from_opt}).
The mean $F_2$ is the same,
but the standard deviation is of order $z$,
so that up to lower order terms, 
while the typical fitness of an individual in the original population is
$\fit_0 = \exp(-(\sigma_s/\gamma)^2)$;
of an $F_1$ is
$\fit_1/\fit_0 = \exp(-(c_1 \sigma_N^2 T/N_e)^2)$;
and of an $F_2$ is
$\fit_2/\fit_0 = \exp(- T/(N_e\gamma^2))$.


%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Parameter estimates for real systems}

To translate these results into real predictions, we need to know
the strength of stabilizing selection on the phenotype,
and the amount (and structure) of heritable variation in the genotype.
These are known at best only roughly \citep{felsenstein1988phylogenies},
so we aim for order-of-magnitude estimates.

%% amount of heritable variation
We quantify (roughly) the amount of heritable variation
by $\sigma^2$, the genetic variance present in a population in a typical entry of $A$.
The coefficient $A_{ij}$ measures how much the rate of net production of $i$ changes
per change in concentration of $j$.
It is generally thought that regulatory sequence change contributes much more to inter- and intraspecific variation
than does coding sequence change affecting molecular structure \citep{schmidt2010vertebrate}.
In the context of transcription factor networks this may be affected 
not only by the binding strength of molecule $j$ to the promoter region of gene $i$
but also the effects of other transcription factors (e.g., cooperativity)
and local chromatin accessibility \citep{stefflova2013cooperativity}.
For this reason, 
the mutational target size for variation in $A_{ij}$ may be much larger than the dozens of base pairs
typically implicated in the handful of binding sites for transcription factor $j$ of a typical promoter region,
and single variants may affect many entries of $A$ simultaneously.
% (Recall that although these are best modeled through nonlinear terms,
% by linearizing we essentially consider first-order effects.)
On the other hand, a diverse set of buffering mechanisms are thought to contribute to phenotypic stability
in the presence of substantial molecular noise \citep{canalization,buffering},
suggesting that substantial variation in the micro-scale dynamics we consider here
may be necessary to produce relevant phenotypic effects downstream.
\plr{replace "micro-scale" with sthg else or discuss earlier}

%% empirical data on sigma
The amount and structure of this standing variation is established over long time scales
by many factors, including
mutation-selection balance, 
shifts in the phenotypic optimum,
and/or spatial variation in the optimum \citep{hansen1996translating}.
Quantitative genetics models of mutation-selection balance 
predict precise levels and structure of standing variation \citep{kimura_mutsel,lande_mutsel,lande1981models},
but it is unclear how well these predictions match reality \citep{johnson_barton}
and how much they are expected to change over time \citep{arnold_changing_G}.
However, empirical work allows us to estimate at least the rough magnitude of variation.
Differences in $A_{ij}$ due to a sequence change are hard to measure,
but variation in both transcription factor binding site occupancy
and expression levels (e.g., cis-eQTL) have been measured in various systems.
However, variation in binding site occupancy may overestimate variation in $A$,
since it does not capture buffering effects (if for instance only one site of many needs be occupied for transcription to begin),
and variation in expression levels measures changes in steady-state concentration (our $\kappa_i$) rather than the \emph{rate} of change.
Nonetheless,
\citet{kasowski2010variation} found differential occupancy in 7.5\%
of binding sites of a transcription factor (p65) between human individuals.
\citet{verlaan2009targeted} showed that cis-regulatory variation
accounts for around 2--6\% of expression variation in human blood-derived primary cells,
while \citep{lappalainen2013transcriptome} found that human population variation 
explained about 3\% of expression variation.
\plr{Get some data from at least one other species in here!}
Taken together, this suggests that variation in the entries of $A$
may be on the order of 1\% between individuals of a population --
doubtless varying substantially between species and between genes.

%% argument for strength of selection
It seems certain that selection in most species is not so strong that intra-population variation
is strongly deleterious,
so that if $u$ is the typical scale on which selection acts,
then $u > \sigma$.  
\plr{do we call this $\beta$ or $u$?}
However,
a range of studies \plr{find them} have found evidence for weak stabilizing selection
on regulatory SNPs and cis-eQTL.
For instance, 
\citet{maria_and_sergey} \plr{(others?)} found evidence that large-effect regulatory mutations are weakly selected against in Drosophila.
This suggests that 
the strength of selection on phenotype is sufficient to weakly constrain regulatory variation,
so that perhaps $\sigma$ and $u$ are relatively close.
This is as would be expected if available variation is held in check by mutation--selection balance
rather than genetic drift.
A conservative estimate would be that $u = 5 \sigma$;
taking $\sigma=.01$ as above, this suggests that changes in phenotype of 5\% are sufficient
to effect a noticeable drop in fitness.
\plr{BUT $\sigma$ IS VARIATION IN $A$ NOT PHENOTYPE}

We have guessed that within a population, 
the entries of $A$ vary by around 1\%, 
at least for networks whose function is strongly constrained.


\section*{The rate of speciation under neutrality in allopatric populations}

\plr{separate out here (or above) the difference in genotype of hybrids from the optimum set, the difference in phenotype, and the difference in fitness.}

%In general terms, if fitness $\fit$ is some function $g$ of phenotypic divergence from an optimal scaled by the strength of selection $\beta$,
%\begin{align*}
%  \fit = g \left( \frac{\phi - \phi_{0}}{\beta} \right)
%\end{align*}
%\plr{sentences don't begin with "And"}
%And a population $P$ is normally distributed around a mean gene network architecture $A(\tau)$ and $\sigma^{2}$ is the intra-population regulatory variance,
%\plr{what is $T$?}
%\begin{align*}
%  P_{T} \sim \mathcal{N} \left(A(\tau), T \frac{\sigma^{2}}{N_{e}}\right)
%\end{align*}
%the population mean architecture will move at rate
%\plr{what is $\Delta \tau$? Also, if we reframe in terms of $V$ not $\tau$ above will need to reframe this.}
%\begin{align*}
%  \Delta \tau = \sigma \sqrt{\frac{T}{N_{e}}}
%\end{align*}
%where $T$ is time measured in generations. 
%USING GENERAL MATH FROM MULTIVARIATE TAYLOR SERIES.... we know that the phenotype of a first generation $F_{1}$ hybrid diverges from that of its parents (assuming both parents are at an optimum and phenotypically equivalent) quartically as a function of $\Delta \tau$. 
%Thus the squared difference of it's phenotype will be,
%\begin{align*}
%  \Delta \Phi^{2}_{1} = c_{1}^{2} \sigma^{4} \left( \frac{T}{N_{e}} \right)^{2}
%\end{align*}
%and $F_{1}$ fitness will decline linearly with respect to $\frac{T}{N_{e}}$. ARGUMENT FOR WHY $\sigma^{2} c_{1} \sim \beta$ HERE.
%   \begin{align*}
%     \Delta \fit_{1} = \left( \frac{\Delta \Phi_{1}}{\beta} = \frac{c_{1} \sigma^{2} \frac{T}{N_{e}}}{\beta} \approx \frac{T}{N_{e}} \right)
%\end{align*}
% It follows FROM THE SAME MATH ABOUT THE MULTIVAR TAYLOR that the phenotypes of second generation $F_{2}$ hybrid crosses diverges quadratically as a function of $\Delta \tau$, 
%\begin{align*}
%  \Delta \Phi^{2}_{2} &= c_{2}^{2} \left( \sigma \sqrt{\frac{T}{N_{e}}} \right)^{2} = c_{2}^{2} \frac{T}{N_{e}} \\
%  \Delta \fit_{2} &= \left( \frac{\Delta \Phi_{2}}{\beta} = \frac{c_{2} \sigma \sqrt{\frac{T}{N_{e}}}}{\beta} \approx \sqrt{\frac{T}{N_{e}}} \right)
%\end{align*}
%and that $F_{2}$ fitness drops rapidly -- at approximately rate $\sqrt{\frac{T}{N_{e}}}$. ARGUMENT FOR WHY $c_{2} \sigma \sim \beta$ HERE. 

This suggests that reproductively isolated populations can speciate rapidly -- on timescales much less than $N_{e}$ generations, depending on the specifics of the fitness function. 


%%%%%%%%%%%%%%%


%    \subsection*{Metabolic Network}
%  \begin{align*}
%    \dot{x}(t) &= \begin{bmatrix} 0 & 1 & 0 & -1 \\ 0 & 0 & -1 & -1 \\ -1 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} \text{GAL3} \\ \text{GAL4} \\ \text{GAL80} \\ \text{MIG1} \end{bmatrix}(t) \\ &+ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \\ 0 & 1 & 1 \end{bmatrix} \begin{bmatrix} \text{galactose} \\ \text{glucose} \\ \text{BTM} \end{bmatrix}(t) \\
%        y(t) &= \begin{bmatrix} 0 & 1 & 0 & -1 \end{bmatrix} \vec{x}(t)
%    \end{align*}
%    \begin{align*}
%      H(z) &= \begin{bmatrix} 1 \\ z^{2} + z + 1 \\ z + 1 \end{bmatrix} \frac{1}{z^{3} + z -1}
%    \end{align*}
%
%    \begin{align*}
%      CV &= C \\
%      VB &= B \\
%      V :&= \begin{bmatrix} 1 & 0 & a & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & b & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} , V^{-1} = \begin{bmatrix} 1 & 0 & \frac{-a}{b} & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & \frac{1}{b} & 0 \\ 0 & 0 & 0 & 1\end{bmatrix} \\
%        A(a,b) :&= VAV^{-1} = \begin{bmatrix} -a & a+1 & \frac{a^{2}}{b} & -1 \\ 0 & 0 & \frac{-1}{b} & -1 \\ -b & b & a & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix}
%    \end{align*}
%    The GAL regulatory network in \emph{S. cerevisiae} is modelled here. The activating transcription factor GAL4, which is regulated by the presence of galactose and two other TFs, binds to the promoter region of the GAL regulon -- a cluster of genes regulated by the same promoter encoding 3 enzymes (GAL1, GAL7, and GAL10), for the metabolism of the sugar galactose. The repressing transcription factor MIG1, which is activated by the presence of glucose and other proteins omitted here, also binds to the regulon, preventing the expression of the galactose metabolizing enzymes. This network is one of the most studied gene regulatory networks in yeast, and experiments have already demonstrated significant transcriptional variation among different species and genuses of yeast.
%
%    In the present model, the $A$ matrix encodes the interactions among GAL3, GAL4, GAL80, and MIG1. $B$ interprets the input: the quantity of glucose, galactose, and the basal transcription (BTM) rates impact of TF concentrations. $C$ represents the promoter region of the GAL regulon and is the sum of the influence of GAL4 (activating) and MIG1 (repressing).
%
%    $A(a,b)$ represent alternative network structures that produce identical outputs, with identical input and output transformations -- meaning the regulatory contributions of the sugars and the basasl machinery, as well as the promoter of the GAL regulon are held constant. These interactions may be realized in nature, if the regulatory differences predicted by $A(a,b)$ are biologically/mutationally realizable. Mutations influencing the interactions between GAL4 and GAL80 have been experimentally demonstrated \citep{li2010alterations}.
%
%    \subsection*{Gap Gene Network}


%%%%%%%%%%%%%%%%%%%%%
\section*{Discussion}

%
%\jss{Discussion guidelines:
%\begin{itemize}
%    \item Why is this important/useful?
%    \item What are the assumptions and shortcomings of the research?
%    \item Compare to other studies in the literature. 
%    \item Future directions. 
%    \item Wild speculations?
%    \item Conclusion and overall impact. 
%\end{itemize}}
%

The complexity of biological systems has limited our understanding of their function and evolution. Above we outline an approach, a first step, towards untangling this complexity in reference to function and evolution. This methodology borrows successfully applied tools from engineering and aims to synthesize these with the concepts and tools of molecular and evolutionary biology. 

Theoretical models in evolution and population genetics often lack the molecular details of physiology or of the genotype-phenotype map. 
Here, we offer a tractable and simple model which includes these missing features. 
\plr{this is a claim we are the first to do something.  best not to make these claims (and we aren't the first to make math models of these)}\js{removed the word ``only''}
Further, we provide, in clear mathematical language, an analytical description of phenomena hitherto discussed verbally and conceptually 
(phenogenetic drift \citep{weiss2000phenogenetic}, developmental systems drift \citep{true2001developmental}, biological degeneracy \citep{edelman2001degeneracy}, \emph{etc.}). 
The tractability and relative simplicity of this exposition enables the interested biologist to work out by hand, 
if desired, 
\plr{probably not desired}\js{:p}
the dynamics of a genetic system, as well as perturbations to the system -- an attribute not likely to be found in less tractable models and simulations.

We have suggested an interpretation of system identification: to see it as an evolutionarily neutral manifold, and not simply a computational nuisance. 
We have demonstrated a method to analytically determine the set of all phenotypically invariant gene networks; 
by a simple change of coordinates in the minimal configuration, or more generally by applying the Kalman decomposition in higher dimensions. 
Further, we emphasize that evolution proceeds through this high dimensional space as stochastic coordinate transformation, constrained by sexual reproduction and selection. 
This set is explored over evolutionary time when phenotype is conserved, and can lead to a diverse set of consequences, 
including the accumulation of Dobzhansky-Muller incompatibilities. 
\plr{check original Bateson/DM papers to see if this accords with those defns}\js{I read through Orr's review of those papers, which included excerpts. It seems like the DMI definition is very general and this accords.}
We emphasize that these incompatibilities are a consequence of recombining different, yet functionally equivalent, mechanisms.

Furthermore, using a quantitative genetic approach, we estimated that a genetically variable population will drift in neutral system space 
at a rate determined by its intra-population variation and its effective population size. 
Because mechanistically distinct yet phenotypically equivalent biological systems can fail to produce viable hybrids, we predict allopatric populations to accumulate genetic incompatibilities 
at a rate on the order of $N_{e}$ under reasonable population genetic parameter estimates. 
Additionally we see second-generation hybrid fitness plummet much faster than that of first-generation hybrids. 
\plr{refer to Turelli here}
This is a consequence of combining our mechanistic model with a quantitative genetic one: 
we observe that $F_{1}$ phenotypes diverge quartically, and $F_{2}$ phenotypes quadratically, with evolutionary time. 
\plr{awkward phrasing}
This result is also consistent with Haldane's rule; that if only one hybrid sex is inviable or sterile it is likely the heterogametic sex. 
The consistency comes from gene networks localized to the sex chromosomes functioning as an $F_{2}$ hybrid cross within a diploid $F_{1}$ heterogamete as there is only one sex chromosome.
  
We also suggest that gene networks may not always use their components parsimoniously 
as network size tends to ratchet up in the absence of strong selection against extra parts. 
\plr{probably shouldn't claim this is "unexplored"} \js{I meant unexplored within this paper.Changed wording. Does that work?}
Although we leave this question unexplored, this phenomena may lead to insights on evolvability and developmental innovation. 
Lastly, we show that hybrid gene networks break down as function of genetic distance, and may, in part, explain broad patterns of reproductive isolation among diverse phyla \citep{roux2016shedding}.

\plr{this paragraph isn't very clear} \js{Should I delete it? I guess it's sort of just a vague ``yeah we know this method has shortcomings''.}
As Richard Levins opined, models in population biology face a trade-off among precision, realism, and generality \citep{levins1966strategy}. As Levins expects, any tractable and general model, such as the present one under discussion, will have limitations. Most notable is linearity. It is often stated that life is not linear. This is often true, however, many of the ideas developed here should be generalizable to nonlinear cases (multi-linear systems, say). Further, we see this as a necessary first step in the direction of more life-like nonlinear evolutionary systems theory. Depending on an actual biological system's particularities, its (potential) non-linearity, may buffer or exacerbate effects elucidated in this paper, such as the acquisition of Dobzansky-Muller incompatibilities.

This theoretical framework can easily be applied to other interesting questions in evolutionary biology not tackled presently: such as the evolution of linkage, the necessity of network complexity (does evolution tend towards Rube Goldberg or parsimonious network organization?), evolvability, structure/function inference, and intra-population context dependency of mutational effects, as well as many others.

%\jss{literature comparison -- I think we should delete}
%
%Over the last several years, several different computational approaches have been applied to study reproductive incompatibility and speciation. \citet{tulchinsky} simulated the evolution of a transcription factor and its binding site using a thermodynamic model. Their simulations suggest that the language by which a transcription factor recognizes its binding site can change, and potentially lead to hybrid incompatibility when allopatric populations employ divergent readout languages. This study, despite looking at gene regulation, does not analyze overall gene network architecture -- as we do here -- it only looks at the expression level of a single gene. Furthermore, they report reproductive isolation primarily following directional selection for a change in expression levels in each allopatric population; the evidence for reproductive isolation following balancing selection is much weaker. Johnson and Porter 2000 did not observe any hybrid fitness declines under stabilizing selection -- only under directional selection. 
%\plr{citations:}
%Khatari et al, Tulchinsky et al, and Porter et al, all study hybrid incompatibility from a transcription factor/binding site interaction perspective, not from an overall network architecture perspective. Palmer and Feldman only see hybrid incompatibility in constant environments if the parental populations are relatively poorly adapted initially. Otherwise hybrids between two allopatric populations have fairly high fitnesses.  

\section*{Acknowledgements}
We would like to thank Sergey Nuzhdin, Stevan Arnold, Erik Lundgren, and Hossein Asgharian for valuable discussion.

\bibliographystyle{plainnat}
\bibliography{krefs}
%\end{multicols}

\normalsize

\section*{Examples}
  \begin{example}[Oscillating Gene Network: Cell Cycle Control]\label{ex:oscillator}


    Cellular division is a complicated phenomena, governed by many different processes, however it is agreed that its rhythm is partially controlled by periodic (oscillating) gene transcription \citep{orlando2008global}. Consider a simplified model of oscillating gene transcription. In the present framework periodic expression requires at minimum two interacting genes. 

    Suppose gene-2 up-regulates the transcription of gene-1 and that gene-1 down-regulates the transcription of gene-2 with equal magnitude (of $1$) and relative to each of their concentrations, denoted by $\kappa_{1}$ and $\kappa_{2}$. Furthermore, suppose that only the dynamics of gene-1 are consequential to the cell cycle (perhaps the amount of gene-1 activates another downstream gene network). Lastly suppose that the production of both genes is stimulated by an impulse of a molecule present immediately after division.

\plr{you haven't explained $u(t)$ above. and, isn't it more natural in this set-up to have $B = [1,0]$ than $[1,1]$?}
    If the rate each of these genes is expressed is a linear function of their concentrations, the dynamics of the system are given by
    \begin{align*}
      \dot{\kappa_{1}}(t) &= \kappa_{2}(t) + u(t) \\
      \dot{\kappa_{2}}(t) &= - \kappa_{1}(t) + u(t)
    \end{align*}
    where $\dot{\kappa}$ denotes the time derivative. The initial conditions $\kappa_{1}(0)$ and $\kappa_{2}(0)$, and the input $u(t)$ then determine the concentrations through time. If we record the regulatory coefficients in the matrix
    \begin{align*}
      A &= \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} ,
    \end{align*}
    and define the column vector $B = \begin{bmatrix} 1 & 1 \end{bmatrix}^{T}$, then in matrix notation the dynamics are
      \begin{align*}
      \dot{\kappa} &= A \kappa(t) + B u(t) .
      \end{align*}
      Since only the dynamics of gene-1 are directly relevant to biological function, the dynamics of interest are given by
      \begin{align*}
      \phi(t) &= C \kappa (t)
      \end{align*}
      where the row vector $C$ is defined as $C = \begin{bmatrix} 1 & 0 \end{bmatrix}$. 
      (If the dynamics of both genes were physiologically relevant to the cell cycle, we would set $C$ to be the identity matrix).
  %  \begin{align*}
  %    A = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} , \qquad B = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \qquad C = \begin{bmatrix} 1 & 0 \end{bmatrix}
  %  \end{align*}
 %       The oscillatory system $\Sigma$ is thus given as
 %   \begin{align*}
 %     \Sigma = \left \{ \begin{array}{ll} \dot{\kappa}(t) &= \begin{bmatrix} 
 %       0 & 1 \\ 
 %      -1 & 0 
 %       \end{bmatrix} \kappa(t) + \begin{bmatrix} 1 \\ 1 \end{bmatrix} u(t) \\ 
 %         \phi(t) &= \begin{bmatrix} 1 & 0 \end{bmatrix} \kappa(t) \end{array} \right.
 %    \end{align*}
      
      Since the input is simply an impulse, its phenotype is equivalent to its impulse response
      \begin{align*}
        \phi(t) = h(t) = \sin(t) + \cos(t)  .
      \end{align*}

    \begin{figure}[H]
       % \begin{center} 
      \centering
         \begin{tabular}{cc}
            \begin{tikzpicture}
            \begin{scope}[every node/.style={circle,thick,draw}]
                \node (A) at (0,0) {$\kappa_{1}$};
                \node[dashed] (B) at (3,0) {$\kappa_{2}$};
                \node[shape=rectangle] (U) at (1.5,2) {input};
                \node[shape=rectangle] (y) at (1.5,-2) {output};
            \end{scope}

            \begin{scope}[>={Stealth[black]},
                          every node/.style={fill=white,circle},
                          every edge/.style={draw=black, thick}]
                \path [->, >=Rectangle] (A) edge[bend left] node {\tiny $-1$} (B);
                \path [->] (B) edge[bend left] node {\tiny $1$} (A); 
                \path[->] (U) edge[dashed] node {\tiny $1$} (A);
                \path[->] (U) edge[dashed] node {\tiny $1$} (B);
                \path[->] (A) edge[dashed,bend right] node {\tiny $1$} (y);
            \end{scope}
            \begin{scope}[>={Stealth[black]},
                          every edge/.style={draw=black, thick}]
                %\path [->] (A) edge[loop left] node {\tiny $\lambda_{1}$} (A);
                %\path [->] (B) edge[loop left] node {\tiny $\lambda_{2}$} (B);
            \end{scope}

            \end{tikzpicture} &
       % \end{center}
      %  \caption{
      %      Diagram of Example \ref{ex:osc} in the text.
      %      \plr{explain what arrows mean if nec}
      %      \label{fig:oscillator_diagram}}
   % \end{figure}
   % \begin{figure}[H]
    %  \centering
      \includegraphics[width=0.5\textwidth, height=0.25\paperheight]{osc_impulse}
   \end{tabular}
  %  \end{center}
      \caption{(Left) Graphical representation of the cell cycle control gene network, and (right) plot of the phenotype $\phi(t)$ against time $t$.}
    \end{figure}
      We return to the evolution of such a system below.
  \end{example}

  \begin{example}[All Phenotypically Equivalent Cell Cycle Control Networks] \label{ex:all_osc}

%\plr{this is same as above, so reference instead of writing out again?}
%    Let
%    \begin{align*}
%      A(0) = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}, \qquad B = \begin{bmatrix} 1 & 1 \end{bmatrix}^{T}, \qquad C = \begin{bmatrix} 1 & 0 \end{bmatrix},
%    \end{align*}
%    with $V(\tau)$ preserving both $B$ and $C$, then
    The set of all two-gene regulatory networks phenotypically equivalent to the cell cycle control network in \ref{ex:oscillator} is given by
    \begin{align*}
      A(\tau) &= \begin{bmatrix} 1 & 0 \\ \tau & 1-\tau \end{bmatrix} \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ \frac{\tau}{\tau-1} & \frac{-1}{\tau-1} \end{bmatrix}  \qquad \forall \ \tau \neq 1 %\\
     %   B &= \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \qquad C = \begin{bmatrix} 1 & 0 \end{bmatrix} \\
       %   h(t) &= \sin(t) + \cos(t) \qquad \forall \ \tau \neq 1
    \end{align*}
    \begin{figure}[H]
    \centering
% Nice tikz!!!
\begin{tikzpicture}
\begin{scope}[every node/.style={circle,thick,draw}]
  \node (A) at (0,0) {$\kappa_{1}$};
    \node[dashed] (B) at (4,0) {$\kappa_{2}$};
    \node[shape=rectangle] (U) at (2,2) {input};
    \node[shape=rectangle] (y) at (2,-2) {output};
\end{scope}

\begin{scope}[>={Stealth[black]},
              every node/.style={fill=white,circle},
              every edge/.style={draw=black, thick}]
    \path [->, sloped] (A) edge[bend left] node {\tiny $2 \tau + (\tau-1)^{-1}$} (B);
    \path [->, sloped] (B) edge[bend left] node {\tiny $-(\tau-1)^{-1}$} (A); 
    \path[->] (U) edge[dashed] node {\tiny $1$} (A);
    \path[->] (U) edge[dashed] node {\tiny $1$} (B);
    \path[->] (A) edge[dashed, bend right] node {\tiny $1$} (y);
\end{scope}
\begin{scope}[>={Stealth[black]},
              every edge/.style={draw=black, thick}]
    \path [->] (A) edge[loop left] node[sloped, anchor=center, above] {\tiny $1 + (\tau-1)^{-1}$} (A);
    \path [->] (B) edge[loop right] node[sloped, anchor=center, above] {\tiny $-1 - (\tau-1)^{-1}$} (B);
\end{scope}

\end{tikzpicture}
    \caption{A graphical depiction of the set of all externally equivalent cell cycle control networks, $A(\tau)$. 
        $\tau$ can take any value \plri{sentence fragment}}
    \end{figure}
 % \end{example}

 % \begin{example}[External Equivalence does not imply internal equivalence]
   Despite the phenotypic equivalence of all instantiations of $A(\tau)$, the internal dynamics, or kryptotypes, vary as a function of $\tau$. 
    Gene-1 dynamics (blue) are equivalent for network architectures $A(0)$ and $A(2)$, however the dynamics of gene-2 (orange) differ with $\tau$.
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=0.25\paperheight]{osc_A0_A2_both_compare}
    \caption{Gene-1 (blue) and gene-2 (orange) dynamics for $A(0)$ (top) and $A(2)$ (bottom). Both (top and bottom) gene-1 dynamics are given by $ \kappa_{1} = \sin(t) + \cos(t)$, and gene-2 by $\kappa_{2} = \cos(t) - \sin(t)$ (top) and $\kappa'_{2} = \cos(t) + 3 \sin(t)$ (bottom).}
  \end{figure}
\end{example}

  \begin{example}[Hybrid Incompatibility in an Oscillating Gene Network] \label{ex:hybrid_osc}

Here we compare the phenotypes for F2 hybrids formed by crossing oscillators $A(2)$ with $A(2.01)$, $A(2.1)$, and $A(2.5)$ ($B$ and $C$ are the same as above). 
Each $A$ is phenotypically identical ($\phi(t) = \sin(t) + \cos(t)$), however some of the hybrids exhibit markedly different dynamics. 

%        \begin{align*}
%         A(2) = \begin{bmatrix} 2 & -1 \\ 5 & -2 \end{bmatrix} &\qquad A(2.01) = \begin{bmatrix} 1.9901 & -0.9901 \\ 5.0101 & -1.9901 \end{bmatrix} \\
%          A(2.1) = \begin{bmatrix} 1.9091 & -0.9091 \\ 5.1091 & -1.9091 \end{bmatrix} &\qquad A(2.5) = \begin{bmatrix} 1.6667 & -0.6667 \\ 5.6667 & -1.6667 \end{bmatrix}
%     \end{align*} 

\begin{alignat*}{2}
      A(2) &= \left[\begin{array}{cc} 2 & -1 \\[6pt] 5 & -2 \end{array}\right] &&\qquad A(2.01) = \left[\begin{array}{cc} 2 -\frac{1}{101} & -1 + \frac{1}{101} \\[6pt] 5 + \frac{1}{99} & -2 + \frac{1}{101} \end{array}\right] \\
        A(2.1) &= \left[\begin{array}{cc} 2 - \frac{1}{11} & -1 + \frac{1}{11} \\[6pt] 5 + \frac{6}{55} & -2 + \frac{1}{11} \end{array}\right] &&\qquad A(2.5) = \left[\begin{array}{cc} 2 - \frac{1}{3} & -1 + \frac{1}{3} \\[6pt] 5 + \frac{2}{3} & -2 + \frac{1}{3} \end{array}\right]
\end{alignat*}

%      \begin{figure}[H]
%        \begin{tabular}{cc}
%          \includegraphics[width=0.2\textwidth]{A2_A3_osci_F2_hyb_1}
%          & \includegraphics[width=0.2\textwidth]{A2_A3_osci_F2_hyb_2}
%         \\ \includegraphics[width=0.2\textwidth]{A2_A3_osci_F2_hyb_3}
%          & \includegraphics[width=0.2\textwidth]{A2_A3_osci_F2_hyb_4}
%          \\ \includegraphics[width=0.2\textwidth]{A2_A3_osci_F2_hyb_5}
%          & \includegraphics[width=0.2\textwidth]{A2_A3_osci_F2_hyb_6}
%        \end{tabular}
%      \end{figure}

%    \begin{figure}[H]
%      \centering
%      \includegraphics[width=0.5\textwidth]{A2_A2-1_F2s}
%      \caption{F2s from $A(2)$ and $A(2.1)$.}
%    \end{figure}
%      \begin{figure}[H]
%        \begin{tabular}{cc}
%        \includegraphics[width=0.5\textwidth]{F2s-small} \\
%        \includegraphics[width=0.5\textwidth]{F2s} \\
%        \includegraphics[width=0.5\textwidth]{F2s-large}
%        \end{tabular}
%        \caption{F2s from $A(2)$ and $A(2.1)$.}
%      \end{figure}

\begin{figure}[H]
  \centering
  \begin{tabular}{cc}
    \includegraphics[width=0.5\textwidth, height=0.25\paperheight]{F1_comparison} & 
    \includegraphics[width=0.5\textwidth, height=0.25\paperheight]{F2s_comparison2}
  \end{tabular}
  \caption{
    $F_1$ (left) and $F_2$ (right) hybrids crossing $A(2)$ with $A(2.01)$ (top), $A(2.1)$ (middle), and $A(2.5)$ (bottom).
    Note the difference in scale on the $y$-axis. F2 hybrids display more phenotypic divergence than F1s, on average. Further, some F2s completely fail to oscillate, as seen in an $A(2.5)$ F2 (light blue).
    \plri{make axis labels bigger}
  }
\end{figure}

    \jss{$A(2)$ and $A(2.1)$ differ in regulatory interaction strengths by $5.09\%$. If intra-population regulatory variation is approximately $5\%$, then this level of divergence is expected after $\sqrt{\frac{t}{N_{e}} = 1}$ generations.}
\end{example}

\begin{example}[Metabolic network] \label{ex:metabolic}

\plr{Concentrations are "logarithmic", you mean that $u_1(t) = \log(\text{concentration of molcule 1 at time t})$.}
Consider an organism that can metabolize two different sugars $s_{1}$ and $s_{2}$ (present at logarithmic concentrations $u_{1}$ and $u_{2}$ respectively), 
with enzymes $e_{1}$ and $e_{2}$ (with log concentration denoted as $\phi_{1}$ and $\phi_{2}$). 
Further suppose that one of the sugars $s_{2}$ is the preferred energy source (perhaps it contains significantly more energy than the other sugar or is otherwise more efficiently metabolized). 
The organism may have a gene regulatory network $\mathcal{S}$ that can deploy a situation specific metabolic strategy. 
\plr{I think you argue below that the optimal response is a complete switch from one enzyme to the other in the presence of the second sugar? Should say so here.}
That is depending on both $u_{2}$ and $u_{1}$ the organism will synthesize an appropriate $\phi_{1}$ and $\phi_{2}$. 
Furthermore, consider this system to contain at least two transcription factors, whose log concentrations are given by $\kappa_{1}$, $\kappa_{2}$, $\dots \kappa_{n}$. 

Minimally such a system may have the architecture,
\begin{align*}
    \mathcal{S}_{\min} = \left\{ \begin{array}{ll} \dot{\kappa}(t) &= \begin{bmatrix} 0 & -1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} \kappa_{1} \\ \kappa_{2} \end{bmatrix} (t) + \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} u_{1} \\ u_{2} \end{bmatrix} (t) \\[11pt]
    \phi(t) &= \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} \kappa_{1} \\ \kappa_{2} \end{bmatrix} (t) \end{array} \right.
\end{align*}
$\mathcal{S}_{\min}$ is minimal as its reachability and observability matrices both have rank $=2$, $\text{rank}(\mathcal{R}) = \text{rank}(\mathcal{O}) = 2 = n \implies \min$.

The impulse response matrix of this system is, 
\begin{align*}
    h(t) = \begin{bmatrix} 1 & - t \\ 0 & 1 \end{bmatrix}  .
\end{align*}

The phenotype is, 
\begin{align*}
  \phi(t) = C e^{A t} \kappa(0) + \int_{0}^{t} h(s-t) u(s) ds
\end{align*}
($\kappa(0)$ can be set to something like $\left[ -10, -10 \right]$, assuming the transient transcription factor and enzyme concentrations in the organism are typically quite low).

We can see that changing coordinates on this system (with any invertible $V \in \mathbb{R}^{2 \times 2}$) will find new system architectures, 
as shown before, however, we can also apply the Kalman decomposition to find systems in different dimensions 
-- that is systems that employ more than $2$ transcription factors, yet have the same external dynamics/phenotype. 
\plr{Ah -- this is much simpler if we let $B$ and $C$ change as well!}
This may happen if a system is co-opted from another, or may be a consequence of gene duplication and deletion. 

\plr{where did $\widehat{\mathcal{S}}$ come from? need a lead-in here.}
$\widehat{\mathcal{S}}$ is $4$-dimensional biological system,  
\begin{align*}
    \widehat{\mathcal{S}} = \left\{ \begin{array}{ll}
      \widehat{A} = V \begin{bmatrix} X_{1} & X_{2} \\ 0 & A_{\min} \end{bmatrix} V^{-1}; \quad \widehat{B} = V \begin{bmatrix} X_{3} \\ B_{\min} \end{bmatrix}; \quad \widehat{C} = \begin{bmatrix} 0 & C_{\min} \end{bmatrix} V^{-1} \end{array} \right\}
\end{align*}
where $V$ is any invertible $4 \times 4$ matrix and $X_{j}$ is any $2 \times 2$ matrix.

\plr{the point here is not clear.}
So for example, some system can be wired as follows, and still be input-output equivalent to the minimal metabolic system $\mathcal{S}_{\min}$:
 \begin{align*}
   A' &= \begin{bmatrix}
     1.6923  &  1.5385 &  -2.6154 &  -2 \\
     0.8462  &  0.7692 &  -1.3077 &  -1 \\
     1.2692  &  1.1538 &  -1.9615 &  -1.5 \\
     0.4231  &  0.3846 &  -0.6538 &  -0.5
   \end{bmatrix} \\
   B' &= \begin{bmatrix} 4 & 4 \\ 2 & 2 \\ 3 & 3 \\ 1 & 3 \end{bmatrix} \\
     C' &= \begin{bmatrix}  0.2692  &  0.1538  &  0.0385 &  -0.5 \\
     -0.4231 &  -0.3846 &   0.6538  &  0.5 \end{bmatrix}
 \end{align*}

Despite the present example consisting of a minimal $2 \times 2$ and a non-minimal $4 \times 4$ system, 
any $n$-dimensional system can be constructed using this method 
-- applying a change of coordinates to the Kalman decomposition 
-- to construct a mechanistically different system with identical phenotypic dynamics. 
Depending on the specifics of a system being modeled, one may have to take care to restrict the free parameter values and network architectures to be biologically appropriate. 
\plr{what are you thinking of as being a potential problem here? say so explicitly.}

\end{example}

\appendix

\section{Kalman Decomposition} \label{apx:kalman}
\begin{definition}[Phenotypic equivalence of systems]
    Let $(\kappa(t),\phi(t))$ and $(\bar \kappa(t),\bar \phi(t))$ be the solutions to \eqref{eqn:lti_system}
    with coefficient matrices $(A,B,C)$ and $(\bar A,\bar B,\bar C)$ respectively,
    and both $\kappa(0)$ and $\bar \kappa(0)$ are zero. 
    The systems defined by $(A,B,C)$ and $(\bar A,\bar B,\bar C)$ are
    \textbf{phenotypically equivalent} 
    if
    \begin{align*}
        \phi(t) = \bar \phi(t) \qquad \text{for all} \; t \ge 0.
    \end{align*}
    Equivalently, this occurs if and only if
    \begin{align*}
        h(t) = \bar h(t)  \qquad \text{for all} \; t \ge 0,
    \end{align*}
    where $h$ and $\bar h$ are the impulse responses of the two systems.
\end{definition}

One way to find other systems equivalent to a given one
is by change of coordinates (``algebraic equivalence''):
if $V$ is an invertible matrix, then the systems $(A,B,C)$ and $(VAV^{-1},VB,CV^{-1})$
have the same dynamics because their transfer functions are equal:
\begin{align*}
    CV^{-1}( zI - VAV^{-1})^{-1}VB
    =
    CV^{-1}V( zI - A)^{-1}V^{-1}VB
    =
    C( zI - A)^{-1}B .
\end{align*}
However, the converse is not necessarily true: 
systems can have identical transfer functions without being changes of coordinates of each other.
In fact, systems with identical transfer functions can involve interactions between different
numbers of molecular species.

The set of all systems phenotypically equivalent to a given system $(A,B,C)$ 
is elegantly described using the Kalman decomposition,
which also clarifies the system dynamics? tells us a lot about how it works? \plr{or something}
To motivate this, first note that the input $u(t)$ only directly pushes the system
in directions lying in the span of the columns of $B$.
As a result, different combinations of input can 
move the system in any direction that lies in the \emph{reachable subspace},
which we denote by $\reachable$,
and is defined to be the closure of $\spn(B)$ under applying $A$
(or equivalently, the span of $B, AB, A^2B, \ldots A^{n-1}B$).
Analogously to this, we define
the \emph{observable subspace}, $\mathcal{O}$,
to be the closure of $\spn(C^T)$ under applying $A$.
(Or: $\unobservable$ is the largest $A$-invariant subspace
contained in the null space of $C$;
and $\reachable$ is the largest $A$-invariant subspace contained in the image of $B$.)

If we define
\begin{enumerate}
    \item The columns of $P_\rno$ are an orthonormal basis for $\reachable \cap \unobservable$.
    \item The columns of $P_\ro$ are an orthonormal basis of
        the complement of $\reachable \cap \unobservable$ in $\reachable$.
    \item The columns of $P_\nro$ are an orthonormal basis of
        the complement of $\reachable \cap \unobservable$ in $\unobservable$.
    \item The columns of $P_\nrno$ are an orthonormal basis of
        the remainder of $\R^n$.
\end{enumerate}
If we then define
\begin{align*}
    P &= 
    \left[ \begin{array}{c|c|c|c}
        P_\rno & P_\ro & P_\nro & P_\nrno
    \end{array} \right] ,
\end{align*}
then
\begin{align*}
    P^T P
    &=
    \left[ \begin{array}{c|c|c|c}
        I & 0 & 0 & 0 \\
        \hline
        0 & I & U & 0 \\
        \hline
        0 & V & I & 0 \\
        \hline
        0 & 0 & 0 & I 
    \end{array} \right] .
\end{align*}
\plr{Check this.  Can we get $U=V=0$?}

The following theorem can be found in SOME REFERENCE.

\begin{theorem}[Kalman decomposition] \label{thm:kalman}
        For any system $(A,B,C)$ with corresponding Kalman basis matrix $P$,
        the transformed system $(PAP^{-1},PB,CP^{-1})$  has the following form:
        \begin{align*}
            \widehat A = PAP^{-1}
            &=
            \left[ \begin{array}{cccc}
                A_{\rno} & A_{\rno,\ro} & A_{\rno,\nrno} & A_{\rno,\nro} \\
                0 & A_{\ro} & 0 & A_{\ro,\nro} \\
                0 & 0 & A_{\nrno} & A_{\nrno,\nro} \\
                0 & 0 & 0 & A_{\nro}
            \end{array} \right] ,
        \end{align*}
        and
        \begin{align*}
            \widehat B = PB
            &=
            \left[ \begin{array}{cccc}
                B_{\rno} \\
                B_{\ro} \\
                0 \\
                0 
            \end{array} \right] ,
        \end{align*}
        and
        \begin{align*}
            \widehat C = CP^{-1}
            &=
            \left[ \begin{array}{cccc}
                0 & C_{\ro} & C_{\nrno} & 0 
            \end{array} \right] .
        \end{align*}
        The transfer function of both systems is given by
        \begin{align*}
            H(z) = C_{\ro} ( zI - A_{\ro} )^{-1} B_{\ro} .
        \end{align*}
\end{theorem}

In the latter case, we say that the system is \emph{minimal} 
-- there is no equivalent system with a smaller number of species.
Note that this says that any two equivalent minimal systems
are changes of basis of each other.

Since any system can be put into this form,
and once in this form, its transfer function is determined only by 
$C_{\ro}$, $A_{\ro}$, and $B_\ro$,
therefore, the set of all equivalent systems are parameterized by
the dimension $n$,
the choice of basis ($P$),
the remaining submatrices in $\widehat A$, $\widehat B$, and $\widehat C$
(which are unconstrained),
and a invertible transformation of $\spn(P_{\ro})$, which we call $T_\ro$.

\begin{theorem}[Parameterization of equivalent systems]
    Let $(A,B,C)$ be a minimal system.
    \begin{itemize}
        \item[(a)]
            Every equivalent system is of the form given in Theorem \ref{thm:kalman},
            i.e., can be specified by choosing a dimension, $n$;
            submatrices in $\widehat A$, $\widehat B$, and $\widehat C$ 
            except for $A_\ro=A$, $B_\ro=B$, and $C_\ro=C$;
            and choosing an invertible matrix $P$.

        \item[(b)]
            \plr{conjecture:}
            The parameterization is unique
            if $P$ is furthermore chosen so that 
            each $P_x$ other than $P_\ro$ is a projection matrix,
            and that 
            \begin{align*}
                0
                =
                P_x^T P_y
            \end{align*}
            for all $(x,y)$ except $(\ro,\nrno)$.

    \end{itemize} 
\end{theorem}

\plr{Another way of saying it: pick the $\reachable$ and $\unobservable$ subspaces,
that must intersect in something of the minimal dimension;
then let $P$ be the appropriate basis?}

In some situations we may be interested in only ``network rewiring'',
where $A$ changes while $B$ and $C$ do not.
For instance, 
if all non-regulatory functions of each molecule are strongly constrained,
then $C$ cannot change.
Likewise, if responses of each molecule to the external inputs are not changed by evolution,
then $B$ does not change.


\subsection{Neutral directions from the Kalman decomposition}

The Kalman decomposition above says that any system $(A,B,C)$ can be decomposed into
$(P, \hat A, \hat B, \hat C)$ so that
$$\begin{aligned}
    A &= P^{-1} \hat A P  \\
    B &= P^{-1} \hat B  \\
    C &= \hat C P ,
\end{aligned}$$
and we know precisely how we can change these to preserve the transfer function:
\begin{enumerate}
    \item $P \to P + \epsilon Q$ as long as the result is still invertible,
    \item $\hat A \to A + \epsilon X$ as long as $X$ is zero in the correct places,
    \item $\hat B \to B + \epsilon Y$ as long as $Y$ is zero in the correct places,
    \item $\hat C \to C + \epsilon Y$ as long as $Z$ is zero in the correct places.
\end{enumerate}
By taking $\epsilon \to 0$, these tell us the local directions we can move a system $(A,B,C)$ in.
All statements below are up to first order in $\epsilon$,
omitting terms of order $\epsilon^2$.

First, since $(P + \epsilon Q)^{-1} = P^{-1} + \epsilon P^{-1} Q P^{-1}$,
modifying $P \to P + \epsilon Q$ changes
$$\begin{aligned}
    A 
        &\to A + \epsilon P^{-1} \hat A Q - \epsilon P^{-1} Q P^{-1} \hat A P \\
        &= A + \epsilon \left(A P^{-1} Q - P^{-1} Q A\right) , \\
    B
        &\to B - \epsilon P^{-1} Q B \\
    C
        &\to C + \epsilon C P^{-1} Q .
\end{aligned}$$
Since $P$ is invertible and $Q$ can be anything (if $\epsilon$ is small enough),
this allows changes in the direction of an arbitrary $W$:
$$\begin{aligned}
    A 
        &= A + \epsilon \left(A W - W A\right) , \\
    B
        &\to B - \epsilon W B \\
    C
        &\to C + \epsilon C W .
\end{aligned}$$

Then, $\hat A \to A + \epsilon X$  does
$$\begin{aligned}
    A \to A + \epsilon P^{-1} X P 
\end{aligned}$$
and $\hat B \to B + \epsilon Y$ does
$$\begin{aligned}
    B \to B + \epsilon P^{-1} Y
\end{aligned}$$
and $\hat C \to C + \epsilon Z$ does
$$\begin{aligned}
    C \to C + \epsilon Z P .
\end{aligned}$$
These degrees of freedom look like they depend on $P$, 
which is not unique,
but for any two choices of $P$ there are corresponding choices of $X$
that give the same actual change in $A$ (and likewise for $Y$ and $Z$).


Therefore, this gives us an upper bound on the number of degrees of freedom,
in terms of the dimensions of the blocks in the Kalman decomposition ($n_\ro$ etc)
and the dimensions of $B$ and $C$ ($n_B$ and $n_C$ respectively):
namely, for $W$, $X$, $Y$, and $Z$ respectively:
$$\begin{aligned}
    n^2 
    + (n_{\rno} + n_\ro n_\nro + n_\nrno(n_\nrno + n_\nro) + n_\nro^2)
    + n_B n_\rno
    + n_C n_\nrno .
\end{aligned}$$
However, some of these may be redundant.
For instance, changing $P$ in the direction of 
a $Q$ that satisfies both $A P^{-1} Q = P^{-1} Q A$ and $C P^{-1} Q = 0$
is equivalent to changing $B$ by $Y = QB$.

\section{Genetic drift with a multivariate trait}
\label{ss:quant_gen}

For completeness, we provide a brief argument of how the population mean
moves under genetic drift
with a quantitative genetics model,
as in \citet{lande1981models} or \citet{hansen1996translating}.
These ignore details of the underlying genetic basis,
but developing a more accurate model is beyond the scope of this paper.

\paragraph{Completing the square}
First note that 
\begin{align*}
    (x-y)^T A (x-y)
    &=
    x^T A \left( x - 2y \right) + y^T A y ,
\end{align*}
and so
\begin{align*}
    (x-y)^T A (x-y) + x^T B x
    &=
    x^T (A + B) \left( x - 2 (A + B)^{-1} A y \right) + y^T A y \\
    &=
    \left( x - (A + B)^{-1} A y \right)^T
    (A + B)
    \left( x - (A + B)^{-1} A y \right)
    + \text{(terms that don't depend on $x$)} .
\end{align*}
Therefore, if $f(x;\Sigma,y)$ is the density of a Gaussian with mean $y$ and covariance matrix $\Sigma$
then substituting $A=\Sigma^{-1}$ and $B=U^{-1}$ above,
\begin{align*}
    \frac{ f(x;\Sigma,y) f(x;U,0) }{\int_x f(z;\Sigma,y) f(z;U,0) dz}
    &=
    f(x; (\Sigma^{-1} + U^{-1})^{-1}, (\Sigma^{-1}+U^{-1})^{-1} \Sigma^{-1} y) .
\end{align*}

Now suppose that the population is distributed in genotype space
as a Gaussian with covariance matrix $\Sigma$ and mean $y$.
Selection has the effect of multiplying this density by the fitness function and renormalizing,
so that if expected fitness of $x$ is proportional to $f(x;U,z)$
then the above argument shows that the next generation will be sampled from a Gaussian distribution
with covariance matrix $(\Sigma^{-1} + U^{-1})^{-1}$ 
and mean $z + (\Sigma^{-1}+U^{-1})^{-1} \Sigma^{-1} (y-z)$
Taking a sample of size $N$ to construct the next generation 
will produce something close to this but with a slightly (stochastically) deviating mean.
The next generation's mean is drawn from a Gaussian distribution with mean
with covariance matrix $(\Sigma^{-1} + U^{-1})^{-1}/N$ 
and mean $z + (\Sigma^{-1}+U^{-1})^{-1} \Sigma^{-1} (y-z)$.

Roughly, what is this doing?
Suppose that the population mean differs from the optimum by $\epsilon$,
that $\Sigma = \sigma^2 I$ and $U = I/\beta^2$ (so, stablizing selection happening on a distance scale of $\beta$).
Then the population mean gets closer to the optimum on average, moving to
$\epsilon/(1 + \sigma^2 \beta^2)$
and adds noise of size $(1/\beta) \sigma/\sqrt{N \sigma^2 + N 1/\beta^2}$.
At equilibrium, these two movements will be of the same order,
so that $\epsilon$ is of order $(\sigma/\sqrt{N}) \sqrt{1+\sigma^2 \beta^2}$.


\section{Away from the optimum}
\label{apx:away_from_opt}

Let two points  on $\optx$ be $x_1$ and $x_2$, let $\bar x = (x_1+x_2)/2$, and let $z=(x_2 - x_1)/2$.
Then with $D \Phi$ and $D^2 \Phi$ the first and second derivatives of $\Phi$, respectively,
then Taylor expanding about $x_1$ and $x_2$ finds that
\begin{align*}
    \Phi(\bar x) 
    &= \Phi(x_1) + D\Phi(x_1) \cdot z + \frac{1}{2} z^T D^2 \Phi(x_1) z + O(\|z\|^3) \\
    &= \Phi(x_2) - D\Phi(x_2) \cdot z + \frac{1}{2} z^T D^2 \Phi(x_2) z + O(\|z\|^3) .
\end{align*}
Now, since $\Phi(x_1) = \Phi(x_2) = \optph$ and
\begin{align*}
    D\Phi(x_2) = D\Phi(x_1) + 2 z^T D^2 \Phi(x_1) + O(\|z\|^2), \quad \text{and} \\
    D^2\Phi(x_2) = D^2\Phi(x_1) + O(\|z\|), \quad \text{and} \\
\end{align*}
adding together the two equations above and dividing by two gets that
\begin{align*}
    \Phi(\bar x) 
    &= \optph - \frac{3}{2} z^T D^2 \Phi(x_1) z + O(\|z\|^3) .
\end{align*}



\section{Differentiating the fitness function}
\label{apx:H_calc}

Suppose that $\rho(t) \ge 0$ is a weighting function on $[0,\infty)$
so that fitness is a function of $L^2(\rho)$ distance of the impulse response from optimal.
With $A_0$ a representative of the optimal set:
\begin{equation}
    \begin{aligned}
        D(A) 
        &:= 
        \int_0^\infty \rho(t) \left| h_A(t) - h_{A_0}(t) \right|^2 dt \\
        &:= 
        \int_0^\infty \rho(t) \left| C e^{At} B - C e^{A_0 t} B \right|^2 dt \\
        &= 
        \int_0^\infty \rho(t) \left| C \left( e^{At} - e^{A_0 t} \right) B \right|^2 dt \\
        &= 
        \int_0^\infty \rho(t) C \left( e^{At} - e^{A_0 t} \right) B B^T \left( e^{At} - e^{A_0 t} \right)^T C^T dt
    \end{aligned}
\end{equation}
How does this change with $A$?
Since
\begin{equation}
  \begin{aligned}
      \frac{d}{du} e^{(A+uZ)t} \vert_{u=0}
      &=
      \int_0^t e^{As} Z e^{A(t-s)} ds, 
  \end{aligned}
\end{equation}
we have that
\begin{equation}
  \begin{aligned}
      \frac{d}{du} D(A+uZ)\vert_{u=0}
      &=
        2 \int_0^\infty \rho(t) C \left( \int_0^t e^{As} Z e^{A(t-s)} ds \right) B B^T \left( e^{At} - e^{A_0 t} \right)^T C^T dt \\
      &=
        2 \int_0^\infty \rho(t) C \left( \int_0^t e^{As} Z e^{A(t-s)} ds \right) B \left( h_A(t) - h_{A_0}(t) \right)^T dt 
  \end{aligned}
\end{equation}
and, by differentiating this and supposing that $A$ is on the optimal set,
i.e., $h_A(t)=h_{A_0}(t)$, (so wolog $A=A_0$):
\begin{equation}
  \begin{aligned}
      \calH(Y,Z) &:= \frac{1}{2} \frac{d}{du} \frac{d}{dv} D(A_0+uY+vZ)\vert_{u=v=0} \\
      &=
        \int_0^\infty \rho(t) C 
        \left( \int_0^t e^{A_0 s} Y e^{A_0 (t-s)} ds \right) 
        B B^T 
        \left( \int_0^t e^{A_0 s} Z e^{A_0 (t-s)} ds \right)^T
        C^T dt  .
  \end{aligned}
\end{equation}
Here $\calH$ is the quadratic form underlying the Hamiltonian.
By defining $\Delta_{ij}$ to be the matrix with a 1 in the $(i,j)$th slot
and 0 elsewhere,
the coefficients of the quadratic form is
\begin{equation}
    \begin{aligned}
        H_{ij, k\ell}(A)
        &:=
        \calH(\Delta_{ij}, \Delta_{k\ell}) .
    \end{aligned}
\end{equation}

We could use this to compute the gradient of $D$,
or to get the quadratic approximation to $D$ near the optimal set.
To do so, it'd be nice to have a way to compute the inner integral above.
Suppose that we can diagonalize $A = U \Lambda U^{-1}$.
Then
\begin{equation} \label{eqn:exp_deriv}
  \begin{aligned}
      \int_0^t e^{As} Z e^{A(t-s)} ds 
      &=
      \int_0^t U e^{\Lambda s} U^{-1} Z U e^{\Lambda (t-s)} U^{-1} ds 
  \end{aligned}
\end{equation}
Now, notice that
\begin{equation}
  \begin{aligned}
      \int_0^t e^{s \lambda_i} e^{(t-s) \lambda_j} ds
      &=
      \frac{ e^{t \lambda_i} - e^{t \lambda_j} }{ \lambda_i - \lambda_j } .
  \end{aligned}
\end{equation}
Therefore, 
defining
\begin{equation}
    X_{ij}(t,Z) = \left( U^{-1} Z U \right)_{ij}
      \frac{ e^{t \lambda_i} - e^{t \lambda_j} }{ \lambda_i - \lambda_j } 
\end{equation}
moving the $U$ and $U^{-1}$ outside the integral and integrating we get that
\begin{equation}
  \begin{aligned}
      \int_0^t e^{As} Z e^{A(t-s)} ds 
      &=
      U X(t,Z) U^{-1} .
  \end{aligned}
\end{equation}

Following on from above, we see that if $Z=\Delta_{k \ell}$, then
\begin{equation}
  \begin{aligned}
      X_{ij}^{k\ell}(t) = 
      \frac{ e^{t \lambda_i} - e^{t \lambda_j} }{ \lambda_i - \lambda_j } 
      (U^{-1})_{\cdot k} U_{\ell \cdot},
  \end{aligned}
\end{equation}
where $U_{k \cdot}$ is the $k$th row of $U$,
and so
\begin{equation}
    \begin{aligned}
        H_{ij, k\ell}(A)
        &=
        \int_0^\infty
            \rho(t) C U X^{ij}(t) U^{-1} B B^T (U^{-1})^T X^{k\ell}(t)^T U^T C^T
        dt .
    \end{aligned}
\end{equation}
This implies that
\begin{equation}
    \begin{aligned}
        D(A_0+\epsilon Z)
        &\approx \epsilon^2\sum_{ijk\ell} H^{ij,k\ell} Z_{ij} Z_{k\ell} 
    \end{aligned}
\end{equation}
and so
\begin{equation}
    \begin{aligned}
        D(A_0+\epsilon Z)
        &\approx \epsilon^2\sum_{ijk\ell} H^{ij,k\ell} Z_{ij} Z_{k\ell} 
    \end{aligned}
\end{equation}

By section \ref{ss:quant_gen},
if we set $\Sigma=\sigma^2 I$ and $U=H$,
then a population at $A_0+Z$ experiences a restoring force of strength
$(I + \sigma^2 H^{-1})^{-1} Z$ (treating $Z$ as a vector and $H$ as an operator on these).
If $\sigma^2$ is small compared to $H^{-1}$
then this is approximately $-\sigma^2 H^{-1} Z$.
This suggests that the population mean follows an Ornstein-Uhlenbeck process,
as described (in different terms) in \citet{hansen1996translating}.

\section{Hybrid Vigor}

If a fitness optimum is at the origin $(0,0) \in \R^{2}$, and two populations have drifted away from the optimum by an distance of $r$, then the expected value of their $F_1$ hybrids is,

\begin{align*}
  \mathbb{E}\left[ \left\lVert \frac{1}{2} ((x_1,y_1) + (x_2,y_2))  \right\rVert \right] = \\
  \frac{\sqrt{2}}{8}\left(\sqrt{3\, r^2 - {\sin^{-1}\!\left(r\right)}^2 + r^4 - 2\, r\, \sin^{-1}\!\left(r\right)\, \sqrt{1 - r^2}} + \sqrt{{\sin^{-1}\!\left(r\right)}^2 + 5\, r^2 - r^4 + 2\, r\, \sin^{-1}\!\left(r\right)\, \sqrt{1 - r^2}}\right)
\end{align*} 

If $r < \sim 5.5$ then we will observe hybrid vigor, however if $r$ is sufficiently large, the hybrids will have lower fitness, on average. 

Some $F_2$s will have perfect fitness, and some will be worse off than both parents. \jss{I still need to do the calculations.} For instance if $r=1$ so that parental populations reside on the unit circle, two randomly drifted populations will create $F_1$s a distance of, on average, $0.6684$. Some $F_2$s from this population wil be back at the origin (a quarter of the time), others as far away as $\sqrt{2}$ (a quarter of the time), and some will stay a distance of $1$ away (half the time).


\end{document}
